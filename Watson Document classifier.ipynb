{
    "metadata": {
        "language_info": {
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "name": "python"
        }, 
        "anaconda-cloud": {}, 
        "kernelspec": {
            "name": "python2", 
            "language": "python", 
            "display_name": "Python 2 with Spark 1.6 (Unsupported)"
        }
    }, 
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Classification and Attribution of data", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## 1. Setup\nTo prepare your environment, you need to install some packages and enter credentials for the Watson services.\n\n### 1.1 Install the necessary packages\n\nYou need the latest versions of these packages:<br>\nWatson Developer Cloud: a client library for Watson services.<br>\nNLTK: leading platform for building Python programs to work with human language data.<br>\npython-keystoneclient: is a client for the OpenStack Identity API.<br>\npython-swiftclient: is a python client for the Swift API.<br><br>\n** Install the Watson Developer Cloud package: **", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 1, 
            "source": "!pip install --upgrade watson-developer-cloud", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already up-to-date: watson-developer-cloud in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages\nRequirement already up-to-date: pyOpenSSL>=16.2.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\nRequirement already up-to-date: python-dateutil>=2.5.3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\nRequirement already up-to-date: requests<3.0,>=2.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\nRequirement already up-to-date: pysolr<4.0,>=3.3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\nRequirement already up-to-date: cryptography>=1.9 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\nRequirement already up-to-date: six>=1.5.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\nRequirement already up-to-date: urllib3<1.23,>=1.21.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\nRequirement already up-to-date: idna<2.7,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\nRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\nRequirement already up-to-date: certifi>=2017.4.17 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\nRequirement already up-to-date: cffi>=1.7; platform_python_implementation != \"PyPy\" in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\nRequirement already up-to-date: enum34; python_version < \"3\" in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\nRequirement already up-to-date: asn1crypto>=0.21.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\nRequirement already up-to-date: ipaddress; python_version < \"3\" in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\nRequirement already up-to-date: pycparser in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cffi>=1.7; platform_python_implementation != \"PyPy\"->cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "** Install NLTK: **", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 2, 
            "source": "!pip install --upgrade nltk", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already up-to-date: nltk in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages\nRequirement already up-to-date: six in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from nltk)\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "** Install IBM Bluemix Object Storage Client: **", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 3, 
            "source": "!pip install python-swiftclient", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied: python-swiftclient in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages\nRequirement already satisfied: requests>=1.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from python-swiftclient)\nRequirement already satisfied: futures>=3.0; python_version == \"2.7\" or python_version == \"2.6\" in /usr/local/src/bluemix_jupyter_bundle.v70/notebook/lib/python2.7/site-packages (from python-swiftclient)\nRequirement already satisfied: six>=1.5.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from python-swiftclient)\nRequirement already satisfied: urllib3<1.23,>=1.21.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\nRequirement already satisfied: idna<2.7,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\nRequirement already satisfied: certifi>=2017.4.17 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "** <font color=blue>Now restart the kernel by choosing Kernel > Restart. </font> **", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### 1.2 Import packages and libraries\n\nImport the packages and libraries that you'll use:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 4, 
            "source": "import json\nimport watson_developer_cloud\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding_v1 \\\n  import Features, EntitiesOptions, KeywordsOptions\n    \nimport swiftclient\nimport re\nimport nltk\nfrom nltk import word_tokenize,sent_tokenize,ne_chunk", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 2. Configuration\n\nAdd configurable items of the notebook below\n\n### 2.1 Add your service credentials from Bluemix for the Watson services\n\nYou must create a Watson Natural Language Understanding service on Bluemix.\nCreate a service for Natural Language Understanding (NLU).\nInsert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n\nRun the cell.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 5, 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "scrolled": true, 
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 2.2 Add your service credentials for Object Storage\n\nYou must create Object Storage service on Bluemix.\nTo access data in a file in Object Storage, you need the Object Storage authentication credentials.\nInsert the Object Storage authentication credentials as <i><b>credentials_1</b></i> in the following cell after \nremoving the current contents in the cell. \n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 6, 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 2.3 Global Variables\n\nAdd global variables.\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 7, 
            "source": "# Specify file names for sample text and configuration files\nsampleTextFileName = \"sample_text.txt\"\nsampleConfigFileName = \"sample_config.txt\"\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 2.4 Configure and download required NLTK packages\n\nDownload the 'punkt' and 'averaged_perceptron_tagger' NLTK packages for POS tagging usage.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 8, 
            "source": "nltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[nltk_data] Downloading package punkt to /gpfs/fs01/user/s973\n[nltk_data]     -7d640fb4db0d6f-c5c16a29391b/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /gpfs/fs01/user/s973-7d640fb4db0d6f-\n[nltk_data]     c5c16a29391b/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
                }, 
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "True"
                    }
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "## 3. Classification\n\nWrite the classification related utility functions in a modularized form.\n\n### 3.1 Watson NLU Classification", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 9, 
            "source": "def analyze_using_NLU(analysistext):\n    \"\"\" Call Watson Natural Language Understanding service to obtain analysis results.\n    \"\"\"\n    response = natural_language_understanding.analyze( \n        text=analysistext,\n        features=Features(entities=EntitiesOptions(), \n                          keywords=KeywordsOptions()))\n    return response", 
            "metadata": {
                "scrolled": true, 
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 3.2 Augumented Classification\n\nCustom classification utlity fucntions for augumenting the results of Watson NLU API call", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 10, 
            "source": "def split_sentences(text):\n    \"\"\" Split text into sentences.\n    \"\"\"\n    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n    sentences = sentence_delimiters.split(text)\n    return sentences\n\ndef split_into_tokens(text):\n    \"\"\" Split text into tokens.\n    \"\"\"\n    tokens = nltk.word_tokenize(text)\n    return tokens\n    \ndef POS_tagging(text):\n    \"\"\" Generate Part of speech tagging of the text.\n    \"\"\"\n    POSofText = nltk.tag.pos_tag(text)\n    return POSofText\n\ndef keyword_tagging(tag,tagtext,text):\n    \"\"\" Tag the text matching keywords.\n    \"\"\"\n    if (text.lower().find(tagtext.lower()) != -1):\n        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n    else:\n        return 'UNKNOWN'\n    \ndef regex_tagging(tag,regex,text):\n    \"\"\" Tag the text matching REGEX.\n    \"\"\"    \n    p = re.compile(regex, re.IGNORECASE)\n    matchtext = p.findall(text)\n    regex_list=[]    \n    if (len(matchtext)>0):\n        for regword in matchtext:\n            regex_list.append(regword)\n    return regex_list\n\ndef chunk_tagging(tag,chunk,text):\n    \"\"\" Tag the text using chunking.\n    \"\"\"\n    parsed_cp = nltk.RegexpParser(chunk)\n    pos_cp = parsed_cp.parse(text)\n    chunk_list=[]\n    for root in pos_cp:\n        if isinstance(root, nltk.tree.Tree):               \n            if root.label() == tag:\n                chunk_word = ''\n                for child_root in root:\n                    chunk_word = chunk_word +' '+ child_root[0]\n                chunk_list.append(chunk_word)\n    return chunk_list\n    \ndef augument_NLUResponse(responsejson,updateType,text,tag):\n    \"\"\" Update the NLU response JSON with augumented classifications.\n    \"\"\"\n    if(updateType == 'keyword'):\n        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n    else:\n        if not any(d.get('text', None) == text for d in responsejson['entities']):\n            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n    \n\ndef classify_text(text, config):\n    \"\"\" Perform augumented classification of the text.\n    \"\"\"\n    \n    response = analyze_using_NLU(text)\n    responsejson = response\n    \n    sentenceList = split_sentences(text)\n    \n    tokens = split_into_tokens(text)\n    \n    postags = POS_tagging(tokens)\n    \n    configjson = json.loads(config)\n    for stages in configjson['configuration']['classification']['stages']:\n        print('Stage - Performing ' + stages['name']+':')\n        for steps in stages['steps']:\n            print('    Step - ' + steps['type']+':')\n            if (steps['type'] == 'keywords'):\n                for keyword in steps['keywords']:\n                    for word in sentenceList:\n                        wordtag = keyword_tagging(keyword['tag'],keyword['text'],word)\n                        if(wordtag != 'UNKNOWN'):\n                            print('      '+keyword['tag']+':'+wordtag)\n                            augument_NLUResponse(responsejson,'entities',wordtag,keyword['tag'])\n            elif(steps['type'] == 'd_regex'):\n                for regex in steps['d_regex']:\n                    for word in sentenceList:\n                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n                        if (len(regextags)>0):\n                            for words in regextags:\n                                print('      '+regex['tag']+':'+words)\n                                augument_NLUResponse(responsejson,'entities',words,regex['tag'])\n            elif(steps['type'] == 'chunking'):\n                for chunk in steps['chunk']:\n                    chunktags = chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n                    if (len(chunktags)>0):\n                        for words in chunktags:\n                            print('      '+chunk['tag']+':'+words)\n                            augument_NLUResponse(responsejson,'entities',words,chunk['tag'])\n            else:\n                print('UNKNOWN STEP')\n    \n    return responsejson\n\ndef replace_unicode_strings(response):\n    \"\"\" Convert dict with unicode strings to strings.\n    \"\"\"\n    if isinstance(response, dict):\n        return {replace_unicode_strings(key): replace_unicode_strings(value) for key, value in response.iteritems()}\n    elif isinstance(response, list):\n        return [replace_unicode_strings(element) for element in response]\n    elif isinstance(response, unicode):\n        return response.encode('utf-8')\n    else:\n        return response\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 4. Persistence and Storage", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### 4.1 Configure Object Storage Client", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 11, 
            "source": "auth_url = credentials_1['auth_url']+\"/v3\"\ncontainer = credentials_1[\"container\"]\n\nIBM_Objectstorage_Connection = swiftclient.Connection(\n    key=credentials_1['password'], authurl=auth_url, auth_version='3', os_options={\n        \"project_id\": credentials_1['project_id'], \"user_id\": credentials_1['user_id'], \"region_name\": credentials_1['region']})\n\ndef create_container(container_name):\n    \"\"\" Create a container on Object Storage.\n    \"\"\"\n    x = IBM_Objectstorage_Connection.put_container(container_name)\n    return x\n\ndef put_object(container_name, fname, contents, content_type):\n    \"\"\" Write contents to Object Storage.\n    \"\"\"\n    x = IBM_Objectstorage_Connection.put_object(\n        container_name,\n        fname,\n        contents,\n        content_type)\n    return x\n\ndef get_object(container_name, fname):\n    \"\"\" Retrieve contents from Object Storage.\n    \"\"\"\n    Object_Store_file_details = IBM_Objectstorage_Connection.get_object(\n        container_name, fname)\n    return Object_Store_file_details[1]", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 5. Classify text\nRead the data file for classification from Object Store<br>\nRead the configuration file for augumented classification from Object Store.<br>\nPersist the classification results as JSON file in object store.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 12, 
            "source": "# Load the text from Object Storage\ntext = get_object(container, sampleTextFileName)\n\n# Load the json configuration from Object Storage\nconfig = get_object(container, sampleConfigFileName)\n\n# Print the json configuration\nprint(\"## Using the configuration ##\")\nprint(config)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "## Using the configuration ##\n{\n  \"configuration\": {\n    \"classification\": {\n      \"stages\": [\n        {\n          \"name\": \"Base Tagging\",\n          \"steps\": [\n            {\n              \"type\": \"keywords\",\n              \"keywords\": [\n                {\n                  \"tag\": \"Passion\",\n                  \"text\": \"Science\"\n                },\n                {\n                  \"tag\": \"Subjects\",\n                  \"text\": \"cosmology\"\n                }\n              ]\n            },\n            {\n              \"type\": \"d_regex\",\n              \"d_regex\": [\n                {\n                  \"tag\": \"Date\",\n                  \"pattern\": \"(\\\\d+/\\\\d+/\\\\d+)\"\n                }\n              ]\n            },\n            {\n              \"type\": \"d_regex\",\n              \"d_regex\": [\n                {\n                  \"tag\": \"Email\",\n                  \"pattern\": \"\\\\b[\\\\w.-]+?@\\\\w+?\\\\.\\\\w+?\\\\b\"\n                }\n              ]\n            },\n            {\n              \"type\": \"d_regex\",\n              \"d_regex\": [\n                {\n                  \"tag\": \"PhoneNumber\",\n                  \"pattern\": \"[0-9]{10}\"\n                }\n              ]\n            },\n            {\n              \"type\": \"chunking\",\n              \"chunk\": [\n                {\n                  \"tag\": \"NP\",\n                  \"pattern\": \"NP:{<DT>?<JJ>*<NN>}\"\n                },\n                {\n                  \"tag\": \"NAME\",\n                  \"pattern\": \"NAME:{<NNP>+}\"\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"Domain Tagging\",\n          \"steps\": [\n            {\n              \"type\": \"d_regex\",\n              \"d_regex\": [\n                {\n                  \"tag\": \"Year\",\n                  \"pattern\": \"[0-9]{4}\"\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 13, 
            "source": "# Classify the text\nresponse = classify_text(text, config)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Stage - Performing Base Tagging:\n    Step - keywords:\n      Passion:science\n      Passion:science\n      Subjects:cosmology\n      Subjects:cosmology\n    Step - d_regex:\n      Date:01/08/1942\n    Step - d_regex:\n    Step - d_regex:\n      PhoneNumber:1112223333\n    Step - chunking:\n      NP: an early age\n      NP: a passion\n      NP: science\n      NP: the sky\n      NP: age\n      NP: cosmology\n      NP: amyotrophic lateral sclerosis\n      NP: illness\n      NP: work\n      NP: cosmology\n      NP: science\n      NP: everyone\n      NP: phone\n      NP: email\n      NP: yahoo.com\n      NAME: Stephen Hawking\n      NAME: Oxford\n      NAME: England\n      NAME: Hawking\n      NAME: University\n      NAME: Cambridge\n      NAME: Stephen Hawking\n      NAME: @\nStage - Performing Domain Tagging:\n    Step - d_regex:\n      Year:1942\n      Year:1112\n      Year:2233\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 14, 
            "source": "# replace unicode strings and convert dict to str for storage\nresponse = str(replace_unicode_strings(response))\nprint(\"~~ Text Classification ~~\")\nprint(response)", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "~~ Text Classification ~~\n{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'relevance': 0.96866, 'text': 'Stephen Hawking'}, {'relevance': 0.884134, 'text': 'amyotrophic lateral sclerosis'}, {'relevance': 0.718936, 'text': 'debilitating illness'}, {'relevance': 0.66862, 'text': 'early age'}, {'relevance': 0.575803, 'text': 'cosmology'}, {'relevance': 0.513458, 'text': 'science'}, {'relevance': 0.492845, 'text': 'yahoo.com'}, {'relevance': 0.475153, 'text': 'passion'}, {'relevance': 0.464197, 'text': 'Oxford'}, {'relevance': 0.464044, 'text': 'England'}, {'relevance': 0.45856, 'text': 'sky'}, {'relevance': 0.4566, 'text': 'University'}, {'relevance': 0.456461, 'text': 'Cambridge'}, {'relevance': 0.453992, 'text': 'work'}, {'relevance': 0.45387, 'text': 'phone'}, {'relevance': 0.453855, 'text': 'physics'}], 'language': 'en', 'entities': [{'relevance': 0.846941, 'text': 'Stephen Hawking', 'disambiguation': {'subtype': ['Academic', 'Astronomer', 'AwardNominee', 'AwardWinner', 'BoardMember', 'Scientist', 'FilmActor', 'FilmWriter', 'TVActor'], 'name': 'Stephen Hawking', 'dbpedia_resource': 'http://dbpedia.org/resource/Stephen_Hawking'}, 'type': 'Person', 'count': 5}, {'relevance': 0.202166, 'text': 'University of Cambridge', 'disambiguation': {'subtype': ['Location', 'CollegeUniversity', 'ProcessorManufacturer', 'University'], 'name': 'University of Cambridge', 'dbpedia_resource': 'http://dbpedia.org/resource/University_of_Cambridge'}, 'type': 'Organization', 'count': 1}, {'relevance': 0.200592, 'text': 'Oxford', 'disambiguation': {'subtype': ['AdministrativeDivision', 'PlaceWithNeighborhoods', 'City'], 'name': 'Oxford', 'dbpedia_resource': 'http://dbpedia.org/resource/Oxford'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'England', 'disambiguation': {'subtype': ['PoliticalDistrict', 'AdministrativeDivision', 'GovernmentalJurisdiction', 'Country'], 'name': 'England', 'dbpedia_resource': 'http://dbpedia.org/resource/England'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'shawking@yahoo.com', 'type': 'EmailAddress', 'count': 1}, {'relevance': 0.5, 'text': 'science', 'type': 'Passion', 'count': 1}, {'relevance': 0.5, 'text': 'cosmology', 'type': 'Subjects', 'count': 1}, {'relevance': 0.5, 'text': '01/08/1942', 'type': 'Date', 'count': 1}, {'relevance': 0.5, 'text': '1112223333', 'type': 'PhoneNumber', 'count': 1}, {'relevance': 0.5, 'text': ' an early age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' a passion', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' science', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' the sky', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' cosmology', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' amyotrophic lateral sclerosis', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' illness', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' work', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' everyone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' phone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' email', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' yahoo.com', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' Stephen Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Oxford', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' England', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' University', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Cambridge', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' @', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': '1942', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '1112', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '2233', 'type': 'Year', 'count': 1}]}\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 15, 
            "source": "# Store the classification response in Object Storage\nput_object(container, \"sample_text_classification.txt\", response, \"text\")\n\n# Retrieve classification response from Object Storage\nget_object(container, \"sample_text_classification.txt\")", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "\"{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'relevance': 0.96866, 'text': 'Stephen Hawking'}, {'relevance': 0.884134, 'text': 'amyotrophic lateral sclerosis'}, {'relevance': 0.718936, 'text': 'debilitating illness'}, {'relevance': 0.66862, 'text': 'early age'}, {'relevance': 0.575803, 'text': 'cosmology'}, {'relevance': 0.513458, 'text': 'science'}, {'relevance': 0.492845, 'text': 'yahoo.com'}, {'relevance': 0.475153, 'text': 'passion'}, {'relevance': 0.464197, 'text': 'Oxford'}, {'relevance': 0.464044, 'text': 'England'}, {'relevance': 0.45856, 'text': 'sky'}, {'relevance': 0.4566, 'text': 'University'}, {'relevance': 0.456461, 'text': 'Cambridge'}, {'relevance': 0.453992, 'text': 'work'}, {'relevance': 0.45387, 'text': 'phone'}, {'relevance': 0.453855, 'text': 'physics'}], 'language': 'en', 'entities': [{'relevance': 0.846941, 'text': 'Stephen Hawking', 'disambiguation': {'subtype': ['Academic', 'Astronomer', 'AwardNominee', 'AwardWinner', 'BoardMember', 'Scientist', 'FilmActor', 'FilmWriter', 'TVActor'], 'name': 'Stephen Hawking', 'dbpedia_resource': 'http://dbpedia.org/resource/Stephen_Hawking'}, 'type': 'Person', 'count': 5}, {'relevance': 0.202166, 'text': 'University of Cambridge', 'disambiguation': {'subtype': ['Location', 'CollegeUniversity', 'ProcessorManufacturer', 'University'], 'name': 'University of Cambridge', 'dbpedia_resource': 'http://dbpedia.org/resource/University_of_Cambridge'}, 'type': 'Organization', 'count': 1}, {'relevance': 0.200592, 'text': 'Oxford', 'disambiguation': {'subtype': ['AdministrativeDivision', 'PlaceWithNeighborhoods', 'City'], 'name': 'Oxford', 'dbpedia_resource': 'http://dbpedia.org/resource/Oxford'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'England', 'disambiguation': {'subtype': ['PoliticalDistrict', 'AdministrativeDivision', 'GovernmentalJurisdiction', 'Country'], 'name': 'England', 'dbpedia_resource': 'http://dbpedia.org/resource/England'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'shawking@yahoo.com', 'type': 'EmailAddress', 'count': 1}, {'relevance': 0.5, 'text': 'science', 'type': 'Passion', 'count': 1}, {'relevance': 0.5, 'text': 'cosmology', 'type': 'Subjects', 'count': 1}, {'relevance': 0.5, 'text': '01/08/1942', 'type': 'Date', 'count': 1}, {'relevance': 0.5, 'text': '1112223333', 'type': 'PhoneNumber', 'count': 1}, {'relevance': 0.5, 'text': ' an early age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' a passion', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' science', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' the sky', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' cosmology', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' amyotrophic lateral sclerosis', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' illness', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' work', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' everyone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' phone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' email', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' yahoo.com', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' Stephen Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Oxford', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' England', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' University', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Cambridge', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' @', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': '1942', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '1112', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '2233', 'type': 'Year', 'count': 1}]}\""
                    }
                }
            ], 
            "cell_type": "code"
        }
    ]
}