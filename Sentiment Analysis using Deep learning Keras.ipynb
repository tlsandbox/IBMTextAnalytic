{
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# This notebok demonstrates how to use python NLTK package for Sentiment Analysis.\n\n## 2017 Dec Shilpa Jain", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Install Python NLTK package\n\nNLTK is a natural language toolkit for building programs in Python that work with natural language text.\nWe will use NLTK for this course.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "!pip install nltk --upgrade"
        }, 
        {
            "source": "## Import NLTK and download NLTK book collection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "import nltk\nnltk.download()\n"
        }, 
        {
            "source": "## Cell below will load all the items in the book module that you have just downloaded. When this finishes, we will see the output.\nWe can see from the output that there are 9 pieces of text and 9 sentences loaded. For example, if we\ntype text1, we will see the title of the first piece of text. If we type sent3, we will see the body of the\nthird sentence.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "                                                Text\n0  The \"Big Brother\" of Singapore football will b...\n1  Mahfizur Rahman watched his friends turn to cr...\n2  The going has been tough, but the Football Ass...\n3  Having pushed reigning world and European cham...\n4  SINGAPORE - Registration for the Standard Char...", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The \"Big Brother\" of Singapore football will b...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mahfizur Rahman watched his friends turn to cr...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The going has been tough, but the Football Ass...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Having pushed reigning world and European cham...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SINGAPORE - Registration for the Standard Char...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 1, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 2, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<Text: SINGAPORE - Registration for the Standard Chartered Marathon...>\nDisplaying 1 of 1 matches:\n                                   SINGAPORE - Registration for the Standard Ch\n"
                }
            ], 
            "cell_type": "code", 
            "source": "import nltk\ndocs=[]\nfor idx, row in df_data_1.iterrows():\n    #print (row['Text'])\n    \n    tokens = nltk.word_tokenize(row['Text'])\n    text2 = nltk.Text(tokens)\n    docs.append(tokens)\n#print ((docs))\nprint (text2)\ntext2.concordance('Singapore')"
        }, 
        {
            "source": "##### In NLTK, there is a method called concordance that allows us to search for a word inside a piece of text.\n##### Count method returns the number of times a word occurs in a piece of text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## UsingWord Counts to Obtain an Overview of a Collection\nAssume that you have a large document collection. For example, it could be all the email enquiries\nfrom the customers of a company in a particular month. It could be all the tweets published by a particular\nuser. It could also be all the fictions written by a particular author. Without going through all the documents\ninside the collection, how can you quickly get an idea about the major topics or themes covered by these\ndocuments?\n\nIn NLTK, there is a built-in function called FreqDist() that makes our task very easy.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<Text: SINGAPORE - Registration for the Standard Chartered Marathon...>\n"
                }, 
                {
                    "data": {
                        "text/plain": "nltk.probability.FreqDist"
                    }, 
                    "execution_count": 3, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "from nltk import *\nprint (text2)\nfdist=FreqDist(text2)\ntype(fdist)"
        }, 
        {
            "source": "##### There is a method called most_common() that can be conveniently used to show the most frequent words in a frequency distribution.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[('the', 8),\n ('at', 7),\n ('will', 6),\n ('.', 6),\n (',', 5),\n ('be', 4),\n ('and', 3),\n (')', 3),\n ('event', 3),\n ('on', 3)]"
                    }, 
                    "execution_count": 4, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "fdist.most_common(10)"
        }, 
        {
            "source": "#### Looking at the most frequent words, you realize that they are not so meaningful. Many words are so commonly used everywhere that they do not reveal anything about the particular document or document collection we are looking at. There are a number of ways to address this problem.\n\n#### We will create a new list text2_long_words and add only words with atleast 5 characters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "from nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nstemmer=PorterStemmer()\nimport gensim\nfrom gensim import corpora\nfrom string import punctuation\n \n\n# Input to dictionary is a list of list\n\n\ndef clean_data(text2):\n\n    text2_long_words=[w for w in text2 if len(w)>=5]\n    stop_list=stopwords.words('english')\n    text2_stopremoved=[w for w in text2_long_words if w not in stop_list]\n    text2_stemmed=[stemmer.stem(w) for w in text2_stopremoved]\n    doc=[text2_stemmed]\n    dictionary=corpora.Dictionary(doc)\n    # remove punctuation from each token\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in text2_stemmed]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    return dictionary,tokens\n    \n    "
        }, 
        {
            "execution_count": 6, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['singapor', 'registr', 'standard', 'charter', 'marathon', 'begin', 'friday', 'registr', 'event', 'raffl', 'place', 'outsid', 'raffl', 'place', 'station', 'first', 'peopl', 'regist', 'event', 'receiv', 'limit', 'edit', 'goodi', 'privileg', 'includ', 'discount', 'prioriti', 'signup', 'run', 'clinic', 'transport', 'design', 'pickup', 'point', 'there', 'dedic', 'entri', 'collect', 'counter', 'standard', 'charter', 'give', 'prize', 'watch', 'membership', 'event', 'registr', 'avail', 'onlin', 'offici', 'websit', 'wwwmarathonsingaporecom', 'payoh', 'sport', 'recreat', 'centr']\n"
                }
            ], 
            "cell_type": "code", 
            "source": "dictionary,vocab=clean_data(text2)\n#print (dictionary)\nprint (vocab)\n"
        }, 
        {
            "source": "#### Checking the frequency distribution and most common words again on the new list gives more sensible results and shows the major characters in a book.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "fdist=FreqDist(vocab)\nm=fdist.most_common(10)"
        }, 
        {
            "source": "## Import Brunel library for visualization", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "import brunel"
        }, 
        {
            "source": "### Create a dataframe to visualize the common words as a tag cloud using Brunel package.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "       word  freq\n0     event   3.0\n1   registr   3.0\n2     place   2.0\n3   charter   2.0\n4     raffl   2.0\n5  standard   2.0\n6  prioriti   1.0\n7     centr   1.0\n8     begin   1.0\n9     payoh   1.0\n"
                }
            ], 
            "cell_type": "code", 
            "source": "import pandas as pd\ndf = pd.DataFrame(columns=['word', 'freq'])\nfor i in m:\n    df.loc[len(df)] = i\n    \nprint (df)\n        "
        }, 
        {
            "source": "## Tag cloud of most common words", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    #visid7ba20cea-f502-11e7-af98-002590fb6404.brunel .chart1 .element1 .element {\n\tfont-family: Impact;\n\tfont-size: 200px;\n}\n</style>\n\n<div id=\"controlsid7ba20f92-f502-11e7-af98-002590fb6404\" class=\"brunel\"/>\n<svg id=\"visid7ba20cea-f502-11e7-af98-002590fb6404\" width=\"600\" height=\"600\"></svg>"
                    }, 
                    "output_type": "display_data", 
                    "metadata": {}
                }, 
                {
                    "data": {
                        "application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 0, 0, 0, 0),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'default')\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visid7ba20cea-f502-11e7-af98-002590fb6404_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    vis.append('clipPath').attr('id', 'clip_visid7ba20cea-f502-11e7-af98-002590fb6404_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n    var scale_x = d3.scaleLinear(), scale_y = d3.scaleLinear();\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n    zoom.on('zoom', function(t, time) {\n        t = t || d3.event.transform;\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        interior.attr('transform', d3.zoomTransform(zoomNode));\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1')\n        .attr('transform','translate(' + geom.inner_width/2 + ',' + geom.inner_height/2 + ')'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0)\n          .sort('freq');\n        processed = post(processed, 0);\n        var f0 = processed.field('freq'),\n          f1 = processed.field('word'),\n          f2 = processed.field('#row'),\n          f3 = processed.field('#selection');\n        var keyFunc = function(d) { return f2.value(d) };\n        data = {\n          freq:         function(d) { return f0.value(d.row) },\n          word:         function(d) { return f1.value(d.row) },\n          $row:         function(d) { return f2.value(d.row) },\n          $selection:   function(d) { return f3.value(d.row) },\n          freq_f:       function(d) { return f0.valueFormatted(d.row) },\n          word_f:       function(d) { return f1.valueFormatted(d.row) },\n          $row_f:       function(d) { return f2.valueFormatted(d.row) },\n          $selection_f: function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return f0.value(d.row)+ '|' + f0.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleLinear().domain([1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3])\n        .interpolate(d3.interpolateHcl)\n        .range([ '#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1', '#f8efe8', '#fef0d9', \n          '#fdcc8a', '#fc8d59', '#e34a33']);\n      var color = function(d) { return scale_color(data.freq(d)) };\n      var scale_size = d3.scaleSqrt().domain([0, 3.0000003])\n        .range([ 0.001, 1]);\n      var size = function(d) { return scale_size(data.freq(d)) };\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        // Build the cloud layout\n        var cloud = BrunelD3.cloudLayout(processed, [geom.inner_width, geom.inner_height], zoomNode);\n        function keyFunction(d) { return d.key };\n        main.attr('class', 'diagram cloud');\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element text filled')\n            .style('text-anchor', 'middle').classed('label', true)\n            .text(function(d) { return data.word_f(d) })\n            .style('font-size', function(d) { return (100*size(d)) + '%' })\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .each(cloud.prepare).call(cloud.build)\n            .filter(BrunelD3.hasData)                     // following only performed for data items\n            .style('fill', color);\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('text');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          key:          ['#row'],\n          color:        ['freq'],\n          size:         ['freq']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['freq', 'word'], \n   options: ['numeric', 'string'], \n   rows: [[3, 'event'], [3, 'registr'], [2, 'place'], [2, 'charter'], [2, 'raffl'], [2, 'standard'],\n  [1, 'prioriti'], [1, 'centr'], [1, 'begin'], [1, 'payoh']]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('visid7ba20cea-f502-11e7-af98-002590fb6404');\nv.build(table1);\n\n    });\n});", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "execution_count": 10, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "%%brunel cloud color(freq) size(freq) sort(freq)\nlabel(word) style('font-size:200px;font-family:Impact') legends(none) :: width = 600, height=600"
        }, 
        {
            "source": "## Keras\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "cell_type": "code", 
            "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n "
        }, 
        {
            "execution_count": 12, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "5\n[\"The `` Big Brother '' of Singapore football will be back , but not immediately , and not for long . In an exclusive interview with The New Paper , Persib Bandung striker Noh Alam Shah said he has agreed to sign a short-term deal with former club Tampines Rovers until the end of the season . But the 31-year-old said : `` Beyond that , I feel my future is still in Indonesia . `` I feel really appreciated here . Four Indo clubs already made me offers for the next season , which starts next January . '' The move to Singapore still hinges on whether Tampines can secure his medical documents and International Transfer Certificate from the Indonesia FA before the transfer window closes today , although the Stags are optimistic . If there are no surprises , Alam Shah will return to Singapore after July 11 , after Persib play their final Indonesia Super League ( ISL ) match against champions Sriwijaya . Said the striker : `` Tampines have always been very close to my heart and I 'm thankful to 'Boss ' ( Tampines chairman Teo Hock Seng ) . `` It will be great if we can win another title together . `` But I feel it is only right for me to finish the last three games with Persib because they have been very good to me . `` Then I have to go back to Malang for my personal belongings . I should be a back a few days after July 11 . '' This means Singapore fans will probably catch their first glimpse of the powerful forward on July 17 , when Tampines visit Woodlands Wellington . He will probably line-up alongside another new striker , Serb Sead Hadzibulic , with ageless stalwart Aleksandar Duric also in the mix . Potentially , it is a fearsome combination , and Tampines coach Steven Tan quipped : `` If we want to defend our title , more power better than no power . '' In an eventful seven years with Tampines from 2003 to 2009 , Alam Shah thrilled fans by thumping in more than 100 goals to help the eastern giants win two S-League titles and two Singapore Cups . However , he also embroiled in a few controversial incidents because of his volatile temperament . One of the gravest was his shocking attack on national teammate Daniel Bennett in the 2007 Singapore Cup final against SAFFC , who won 4-3 . Alam Shah , known for his straight-talking nature , had criticised the Beep Test before he left the S-League in 2009 to captain Arema Malang to ISL glory . The mere mention of the compulsory fitness test yesterday - he has to pass it to be able to play for Tampines - was enough to make him bristle . He said : `` I still hold the same views . `` I will have to pass the Beep Test to play in the S-League again . `` If I ca n't pass , I can go back to Indonesia . But the sad thing is , I see so many good footballers who ca n't play because they fail this Beep Test . '' The timing of his return coincides with this year 's Suzuki Cup , which will be co-hosted by Malaysia and Thailand from Nov 14 to Dec 8 . Three-rime champions Singapore have targeted a place in the final and could do with a proven striker . The former national skipper , who has 35 goals in 80 internationals , is the all-time top scorer in the regional competition with 17 goals . Said Alam Shah : `` Of course every player wants to play for his national team , but I will be 32 by then . `` If they call me up , it shows they have a lack of young strikers who can fill the gap . `` I 've already thought of retiring from international football , but if I get a call-up , I will accept and share my experience with the younger players . ''\", \"Mahfizur Rahman watched his friends turn to cricket , golf and soccer over the years in the hopes of earning a living out of sport , but the Bangladesh swimmer is happy with his decision to stick to the water . Mahfizur , whose nickname Sagar means `` sea '' in Bengali , believes years of perseverance have paid off after he was handed a wildcard place to compete at the London Olympics . Despite being crisscrossed by more than 230 rivers , swimming is a minor sport in Bangladesh and the lack of financial reward plays a major part in that . Cricket careers can be extremely lucrative on the subcontinent , while Siddikur Rahman has earned almost US $ 150,000 ( $ 190,000 ) on golf 's Asian Tour this season . Swimming is the poor cousin , says the 19-year-old . `` All we earn is a medal , which is sometimes not even worth 50 taka ( $ 0.75 ) , so it is very difficult to keep us motivated , '' said Mahfizur at Dhaka 's National Swimming Complex , where he trains . `` We know we will gain very little out of it ; still we keep swimming because we love it . '' Mahfizur , who has been at Bangladesh 's lone sports academy ( BKSP ) for more than a decade , has recently been selected by the Bangladesh Navy to become a junior commanding officer . `` When I came to BKSP in 2001 , I saw everybody just wants to be a cricketer . But , frankly , it has never attracted me , '' he said . Mahfizur is the undisputed king of Bangladesh 's freestyle swimming , winning gold medals in the 50 , 100 , 200 , 400 and 1,500 metres in every national meet since 2007 . He struck his first gold medal at senior level in 2005 at the age of 12 . However , getting to the Olympics always seemed like a distant dream as his times had been nowhere near the top level . He clocked 24.82 seconds in the 50 freestyle at the World Championships in Shanghai last year , far behind gold medal winner Cesar Cielo 's 21.52 seconds . `` One morning last February , one of my coaches told me that I was going to swim in the Olympics . I did not believe him and went straight to the Bangladesh Olympic Association , where officials confirmed the news , '' he said . `` I was lost for words for a few seconds because this was unbelievably good news for me . I could not think of it despite being the best in the country for the last few years . '' The International Olympic Committee granted Mahfizur a wildcard place after analysing his performance in domestic events . `` I always knew swimming would give me nothing but honour . And representing the country in an Olympics is the best honour for any athlete , '' he said .\", \"The going has been tough , but the Football Association of Singapore ( FAS ) has reiterated it will continue to work towards improving the S-League . In response to queries from The New Paper , the FAS admitted `` there was a drop in attendance ( figures ) as compared to previous years '' , although it declined to provide figures . Said an FAS spokesman : `` The FAS and the S-League management recognise that more can be done to draw more fans . `` We are always looking at ways to improve the league and recognise areas for improvement such as match attendances , and we are working with all stakeholders to improve in these areas . '' Some S-League clubs with no main sponsors have decided to streamline their operations due to budget constraints ( see main story ) . The FAS noted that clubs have received significantly larger subsidies over the last two years - an increase of about 70 per cent from 2010 . The S-League has also secured funding from the Tote Board for the next four years , which will help in managing the clubs ' operating budgets to allow them to attract better quality foreign players as well as to retain their players . Said the FAS spokesman : `` All S-League clubs have always been operating on a tight annual budget and exercise prudence in their expenditure . This is more evident now in view of more difficult revenue streams in recent years . '' The FAS highlighted several policies that have been put in place to assist clubs to better manage their finances as well as to ensure proper accounting governance . For example , there are shared services for clubs such as group insurance to reduce expenses and the existence of a salary cap for players ensures that club spending is within budget . Without elaborating , the FAS revealed it is working on several initiatives which will be introduced in the second half of the season , while Balestier Khalsa vice-chairman S Thavaneson 's suggestion to increase the number of league matches is also apparently under review . Some fans wonder if the S-League will ever be able to match the sizeable crowds currently turning up for LionsXII matches in the Malaysian Super League . Said the FAS : `` The S-League was commended by the Asian Football Confederation for the quality and competitiveness of the players and participating teams , including the foreign teams . `` We would like to reiterate that the S-League is the key pillar of Singapore football and it is imperative that it stays strong for the betterment of football development in Singapore from the youth levels to the professional league to the national teams .\", \"Having pushed reigning world and European champions Spain for 120 minutes in their Euro 2012 semi-final , Portugal can reflect on a successful showing in Poland and Ukraine . Cesc Fabregas ' winning penalty gave Spain a 4-2 shootout success in Donetsk on Wednesday that sent Vicente del Bosque 's side into the final , but the margins of failure were wafer-thin for beaten Portugal . Had Cristiano Ronaldo not lifted the ball over the crossbar in the final minute of normal time , had Bruno Alves ' fourth Portuguese penalty not hit the bar and bounced back , things could have been very different . Dogged in defence and menacing on the counter-attack , Portugal proved themselves worthy of a place in the last four and although they failed to reach the final , coach Paulo Bento said there was plenty of cause for optimism . `` All that 's left for me is the way that we played and the way we have competed , '' he said . `` Even though we lost the semi-final , that should give us a lot of confidence . `` The country can be proud of the team 's efforts . In the 90 minutes ( normal match time ) we played with a lot of speed and caused Spain a lot of problems . `` That 's why I 'm proud and all of Portugal can be proud . '' Portugal have now won only once in six attempts in major tournament semi-finals -- their last final appearance being their loss to Greece as hosts of Euro 2004 -- but they grew in strength as Euro 2012 progressed . Unfortunate in their opening 1-0 loss to Germany , in which they twice hit the crossbar , they reacted in stirring fashion to beat Denmark 3-2 through an 87th-minute winner from substitute Silvestre Varela . It was then that the Cristiano Ronaldo show began . The 27-year-old produced one of the tournament 's outstanding individual performances in the final Group B game against Netherlands , scoring twice to cancel out Rafael van der Vaart 's opener and sending Portugal into the last eight . He was every bit as irresistible in the quarter-final against a stubbornly defensive Czech Republic side , whose resistance was finally pierced by a 79th-minute header from the Portugal captain . Given Portugal 's pre-tournament form -- 0-0 draws with Poland and Macedonia , followed by a 3-1 loss to Turkey -- and the strength of their group , a semi-final showing represents a commendable achievement . For all the disappointment of the defeat by Spain , the Seleccao have come a long way since the 1-0 loss to Norway in their second Euro 2012 qualifier in September 2010 that cost Carlos Queiroz his job at the helm of the team . `` I think people will be proud of us , for what we achieved here , '' said centre-back Pepe . `` We are a young group and we will have other opportunities in the future . '' There is every reason to expect that the squad will remain largely intact for the 2014 World Cup in Brazil . Prior to the game against Spain , the average age of Portugal 's starting line-up was 26.9 , and their key players should all still be in their prime in two years ' time -- Ronaldo will be 29 , Nani 27 , and Pepe 31 . Bento , 43 , is under contract until 2014 and he has established a sense of calmness and continuity that bodes well for the World Cup qualifying campaign , which starts with a match away to Luxembourg on Sept 7 .\", 'SINGAPORE - Registration for the Standard Chartered Marathon 2012 will begin on Friday , July 6 , at 11am . A registration event will be held at Raffles Place Park , outside Raffles Place MRT Station at the same time . The first 1,000 people to register at the event will receive limited edition goodie bags and race privileges including discounts and priority sign-ups at running clinics , as well as free transportation from designated pick-up points on the day of the race . There will also be a dedicated Race Entry Pack Collection ( REPC ) counter for them . Standard Chartered will be giving away prizes such as watches and gym memberships at the event . Registration will also be made available online via the official website ( www.marathonsingapore.com ) on the same day , or at Toa Payoh Sports & amp ; Recreation Centre ( SRC ) from July 7 .']\n"
                }
            ], 
            "cell_type": "code", 
            "source": "print (len(docs))\ntrain_docs=[]\nfor d in docs:\n    train_docs.append(\" \".join(d))\nprint ((train_docs))"
        }, 
        {
            "execution_count": 13, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "#create the tokenizer\ntokenizer = Tokenizer()\n# fit the tokenizer on the documents\ntokenizer.fit_on_texts(train_docs)"
        }, 
        {
            "execution_count": 21, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 0.        ,  2.70684242,  2.50667111, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  2.58098477,  2.05958598, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  2.77823493,  2.45152986, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  2.77823493,  2.00181507, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  1.93795229,  0.6061358 , ...,  1.25276297,\n         1.25276297,  1.25276297]])"
                    }, 
                    "execution_count": 21, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "# sequence encode\nencoded_docs = tokenizer.texts_to_matrix(train_docs,mode='tfidf')\nencoded_docs"
        }, 
        {
            "execution_count": 22, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 2, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 1, 1, 1]], dtype=int32)"
                    }, 
                    "execution_count": 22, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "# pad sequences\nmax_length = max([len(s.split()) for s in train_docs])\nXtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nXtrain"
        }, 
        {
            "execution_count": 23, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[0 0 1 1 1]\n"
                }
            ], 
            "cell_type": "code", 
            "source": "from numpy import array\n# define training labels\nprint (array([0 for _ in range(2)] + [1 for _ in range(3)]))\nytrain = array([0 for _ in range(2)] + [1 for _ in range(3)])"
        }, 
        {
            "execution_count": 24, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "896"
                    }, 
                    "execution_count": 24, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "# define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size"
        }, 
        {
            "execution_count": 25, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 695, 100)          89600     \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 688, 32)           25632     \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 344, 32)           0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 11008)             0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                110090    \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 225,333\nTrainable params: 225,333\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
                }
            ], 
            "cell_type": "code", 
            "source": "model = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=max_length))\nmodel.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())"
        }, 
        {
            "execution_count": 26, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/10\n0s - loss: 0.6937 - acc: 0.2000\nEpoch 2/10\n0s - loss: 0.6900 - acc: 0.6000\nEpoch 3/10\n0s - loss: 0.6778 - acc: 0.6000\nEpoch 4/10\n0s - loss: 0.6453 - acc: 0.6000\nEpoch 5/10\n0s - loss: 0.5998 - acc: 0.6000\nEpoch 6/10\n0s - loss: 0.5308 - acc: 0.8000\nEpoch 7/10\n0s - loss: 0.4470 - acc: 1.0000\nEpoch 8/10\n0s - loss: 0.3522 - acc: 1.0000\nEpoch 9/10\n0s - loss: 0.2635 - acc: 1.0000\nEpoch 10/10\n0s - loss: 0.1833 - acc: 1.0000\n"
                }, 
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7fbfdc7439e8>"
                    }, 
                    "execution_count": 26, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit network\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)"
        }, 
        {
            "execution_count": 27, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Test Accuracy: 100.000000\n"
                }
            ], 
            "cell_type": "code", 
            "source": "# evaluate\nloss, acc = model.evaluate(Xtrain, ytrain, verbose=0)\nprint('Test Accuracy: %f' % (acc*100))"
        }, 
        {
            "execution_count": 31, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[]]\n<keras.models.Sequential object at 0x7fc05c0cf668>\n"
                }, 
                {
                    "data": {
                        "text/plain": "1.0"
                    }, 
                    "execution_count": 31, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "import numpy as np\nTEXT='I like you'\npre_doc=[]\ndata_pre=[]\ndic,pre_tok=clean_data(TEXT)\npre_doc.append(pre_tok)\nprint (pre_doc)\nfor d in pre_doc:\n    data_pre.append(\" \".join(d))\n#tokens=clean_doc(test)\nMAX_SEQUENCE_LENGTH=695\nprint (model)\nSEQUENCES = tokenizer.texts_to_matrix(data_pre,mode='tfidf')\n#max_length = max([len(s.split()) for s in TEXT])\nDATA = pad_sequences(SEQUENCES, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n#print (DATA)\nPREDICTION = model.predict(DATA,verbose=0)\n#print('result: ' + np.array(LABELS)[PREDICTION.argmax(axis=1)][0])\n\nround(PREDICTION[0,0])\n#model.predict(np.array('I like you'))"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": ""
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "name": "python", 
            "file_extension": ".py", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21"
        }
    }
}